accuracy <- (TP + TN) / (TP+TN+FP+FN)
specificity <- TN / (TN + FP)
sensitivity <- TP / (TP + FN)
presicion
accuracy
specificity
sensitivity
TP <- resultados_mayoria$True_Positive
TN <- resultados_mayoria$True_Negative
FP <- resultados_mayoria$False_Positive
FN <- resultados_mayoria$False_Negative
presicion <- TP / (TP + FP)
accuracy <- (TP + TN) / (TP+TN+FP+FN)
specificity <- TN / (TN + FP)
sensitivity <- TP / (TP + FN)
presicion
accuracy
specificity
sensitivity
TP <- resultados_random$True_Positive
TN <- resultados_random$True_Negative
FP <- resultados_random$False_Positive
FN <- resultados_random$False_Negative
presicion <- TP / (TP + FP)
accuracy <- (TP + TN) / (TP+TN+FP+FN)
specificity <- TN / (TN + FP)
sensitivity <- TP / (TP + FN)
presicion
accuracy
specificity
sensitivity
TP <- resultados_mayoria$True_Positive
TN <- resultados_mayoria$True_Negative
FP <- resultados_mayoria$False_Positive
FN <- resultados_mayoria$False_Negative
presicion <- TP / (TP + FP)
accuracy <- (TP + TN) / (TP+TN+FP+FN)
specificity <- TN / (TN + FP)
sensitivity <- TP / (TP + FN)
presicion
accuracy
specificity
sensitivity
# Proporciones de la clase inclinacion_peligrosa
proporciones <- prop.table(distribucion)
# Mostrar las proporciones
proporciones
# Proporciones de la clase inclinacion_peligrosa
proporciones <- prop.table(distribucion)
# Mostrar las proporciones
proporciones
View(datos_80)
View(datos_validation)
View(resultados)
resultados <- cross_validation(datos_80, 10)
create_folds <- function(data, k) {
# Mezclar los índices de las filas
set.seed(123)  # Fijar semilla para reproducibilidad
indices <- sample(seq_len(nrow(data)))
# Dividir en k partes iguales
folds <- split(indices, cut(seq_along(indices), k, labels = FALSE))
# Asignar nombres de "Fold1", "Fold2", ...
names(folds) <- paste0("Fold", seq_along(folds))
return(folds)
}
library(rpart)
library(caret) # Para calcular métricas
install.packages("caret")
library(rpart)
library(caret) # Para calcular métricas
cross_validation <- function(data, k) {
folds <- create_folds(data, k)
# Inicializar vectores para almacenar métricas
accuracy <- precision <- sensitivity <- specificity <- numeric(k)
for (i in seq_along(folds)) {
# Dividir en conjunto de entrenamiento y test usando los folds
test_indices <- folds[[i]]
train_data <- data[-test_indices, ]
test_data <- data[test_indices, ]
# Entrenar el modelo de árbol de decisión
model <- rpart(inclinacion_peligrosa ~ ., data = train_data, method = "class")
# Realizar predicciones
predictions <- predict(model, test_data, type = "class")
# Calcular métricas
confusion <- confusionMatrix(predictions, test_data$inclinacion_peligrosa, positive = "1")
accuracy[i] <- confusion$overall['Accuracy']
precision[i] <- confusion$byClass['Precision']
sensitivity[i] <- confusion$byClass['Sensitivity']
specificity[i] <- confusion$byClass['Specificity']
}
# Calcular medias y desviaciones estándar
metrics <- data.frame(
Metric = c("Accuracy", "Precision", "Sensitivity", "Specificity"),
Mean = c(mean(accuracy), mean(precision), mean(sensitivity), mean(specificity)),
SD = c(sd(accuracy), sd(precision), sd(sensitivity), sd(specificity))
)
return(metrics)
}
install.packages(ggplot2)
resultados <- cross_validation(datos_80, 10)
resultados <- cross_validation(datos_80, 10)
# Elimina la columna 'ultima_modificacion' antes de la validación cruzada
datos_80_clean <- datos_80 %>% select(-ultima_modificacion)
# Elimina la columna 'ultima_modificacion' antes de la validación cruzada
datos_80_clean <- datos_80 %>% select(-ultima_modificacion)
# Eliminar la columna 'ultima_modificacion' sin el operador %>%
datos_80_clean <- datos_80[, !(names(datos_80) %in% "ultima_modificacion")]
# Ejecuta la validación cruzada con el dataframe limpio
resultados <- cross_validation(datos_80_clean, 10)
# Cargar el paquete dplyr
library(dplyr)
# Eliminar la columna 'ultima_modificacion' antes de la validación cruzada
datos_80_clean <- datos_80 %>% select(-ultima_modificacion)
# Ejecutar la validación cruzada con el dataframe limpio
resultados <- cross_validation(datos_80_clean, 10)
View(datos_80)
# Cargar el paquete dplyr
library(dplyr)
# Eliminar la columna 'ultima_modificacion' antes de la validación cruzada
datos_80_clean <- datos_80 %>% select(-ultima_modificacion)
# Ejecutar la validación cruzada con el dataframe limpio
resultados <- cross_validation(datos_80_clean, 10)
library(rpart)
library(caret) # Para calcular métricas
cross_validation <- function(data, folds) {
# Crear folds
folds_indices <- create_folds(data, folds)
# Inicializar listas para guardar las métricas de cada fold
accuracy_list <- c()
precision_list <- c()
sensitivity_list <- c()
specificity_list <- c()
for (i in seq_along(folds_indices)) {
# Separar los índices de test y entrenamiento para este fold
test_indices <- folds_indices[[i]]
train_data <- data[-test_indices, ]
test_data <- data[test_indices, ]
# Eliminar la columna 'ultima_modificacion' que no es relevante para el modelo
train_data <- train_data %>% select(-ultima_modificacion)
test_data <- test_data %>% select(-ultima_modificacion)
# Asegurarse de que 'inclinacion_peligrosa' es un factor en ambos conjuntos
train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa, levels = c(0, 1))
test_data$inclinacion_peligrosa <- factor(test_data$inclinacion_peligrosa, levels = c(0, 1))
# Entrenar el modelo de árbol de decisión
model <- rpart(inclinacion_peligrosa ~ ., data = train_data, method = "class")
# Realizar predicciones en el conjunto de test
test_data$prediction_class <- predict(model, test_data, type = "class")
# Asegurarse de que ambas columnas son factores con los mismos niveles
test_data$prediction_class <- factor(test_data$prediction_class, levels = c(0, 1))
# Calcular la matriz de confusión
cm <- confusionMatrix(test_data$prediction_class, test_data$inclinacion_peligrosa, positive = "1")
# Guardar las métricas
accuracy_list <- c(accuracy_list, cm$overall["Accuracy"])
precision_list <- c(precision_list, cm$byClass["Precision"])
sensitivity_list <- c(sensitivity_list, cm$byClass["Sensitivity"])
specificity_list <- c(specificity_list, cm$byClass["Specificity"])
}
# Calcular media y desviación estándar de cada métrica
results <- list(
accuracy_mean = mean(accuracy_list, na.rm = TRUE),
accuracy_sd = sd(accuracy_list, na.rm = TRUE),
precision_mean = mean(precision_list, na.rm = TRUE),
precision_sd = sd(precision_list, na.rm = TRUE),
sensitivity_mean = mean(sensitivity_list, na.rm = TRUE),
sensitivity_sd = sd(sensitivity_list, na.rm = TRUE),
specificity_mean = mean(specificity_list, na.rm = TRUE),
specificity_sd = sd(specificity_list, na.rm = TRUE)
)
return(results)
}
# Cargar el paquete dplyr
library(dplyr)
# Eliminar la columna 'ultima_modificacion' antes de la validación cruzada
datos_80_clean <- datos_80 %>% select(-ultima_modificacion)
# Ejecutar la validación cruzada con el dataframe limpio
resultados <- cross_validation(datos_80_clean, 10)
# Cargar el paquete dplyr
library(dplyr)
# Ejecutar la validación cruzada con el dataframe limpio
resultados <- cross_validation(datos_80, 10)
# Imprimir niveles
print(levels(train_data$inclinacion_peligrosa))
library(rpart)
library(caret) # Para calcular métricas
cross_validation <- function(data, folds) {
# Crear folds
folds_indices <- create_folds(data, folds)
# Inicializar listas para guardar las métricas de cada fold
accuracy_list <- c()
precision_list <- c()
sensitivity_list <- c()
specificity_list <- c()
for (i in seq_along(folds_indices)) {
# Separar los índices de test y entrenamiento para este fold
test_indices <- folds_indices[[i]]
train_data <- data[-test_indices, ]
test_data <- data[test_indices, ]
# Eliminar la columna 'ultima_modificacion' que no es relevante para el modelo
train_data <- train_data %>% select(-ultima_modificacion)
test_data <- test_data %>% select(-ultima_modificacion)
# Asegurarse de que 'inclinacion_peligrosa' es un factor en ambos conjuntos
train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa, levels = c(0, 1))
test_data$inclinacion_peligrosa <- factor(test_data$inclinacion_peligrosa, levels = c(0, 1))
# Imprimir niveles
print(levels(train_data$inclinacion_peligrosa))
print(levels(test_data$inclinacion_peligrosa))
# Entrenar el modelo de árbol de decisión
model <- rpart(inclinacion_peligrosa ~ ., data = train_data, method = "class")
# Realizar predicciones en el conjunto de test
test_data$prediction_class <- predict(model, test_data, type = "class")
# Asegurarse de que ambas columnas son factores con los mismos niveles
test_data$prediction_class <- factor(test_data$prediction_class, levels = c(0, 1))
# Calcular la matriz de confusión
cm <- confusionMatrix(test_data$prediction_class, test_data$inclinacion_peligrosa, positive = "1")
# Guardar las métricas
accuracy_list <- c(accuracy_list, cm$overall["Accuracy"])
precision_list <- c(precision_list, cm$byClass["Precision"])
sensitivity_list <- c(sensitivity_list, cm$byClass["Sensitivity"])
specificity_list <- c(specificity_list, cm$byClass["Specificity"])
}
# Calcular media y desviación estándar de cada métrica
results <- list(
accuracy_mean = mean(accuracy_list, na.rm = TRUE),
accuracy_sd = sd(accuracy_list, na.rm = TRUE),
precision_mean = mean(precision_list, na.rm = TRUE),
precision_sd = sd(precision_list, na.rm = TRUE),
sensitivity_mean = mean(sensitivity_list, na.rm = TRUE),
sensitivity_sd = sd(sensitivity_list, na.rm = TRUE),
specificity_mean = mean(specificity_list, na.rm = TRUE),
specificity_sd = sd(specificity_list, na.rm = TRUE)
)
return(results)
}
# Cargar el paquete dplyr
library(dplyr)
# Ejecutar la validación cruzada con el dataframe limpio
resultados <- cross_validation(datos_80, 10)
library(rpart)
library(caret) # Para calcular métricas
cross_validation <- function(data, folds) {
# Crear folds
folds_indices <- create_folds(data, folds)
# Inicializar listas para guardar las métricas de cada fold
accuracy_list <- c()
precision_list <- c()
sensitivity_list <- c()
specificity_list <- c()
for (i in seq_along(folds_indices)) {
# Separar los índices de test y entrenamiento para este fold
test_indices <- folds_indices[[i]]
train_data <- data[-test_indices, ]
test_data <- data[test_indices, ]
# Eliminar la columna 'ultima_modificacion' que no es relevante para el modelo
train_data <- train_data %>% select(-ultima_modificacion)
test_data <- test_data %>% select(-ultima_modificacion)
# Asegurarse de que 'inclinacion_peligrosa' es un factor en ambos conjuntos
train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa, levels = c(0, 1))
test_data$inclinacion_peligrosa <- factor(test_data$inclinacion_peligrosa, levels = c(0, 1))
# Entrenar el modelo de árbol de decisión
model <- rpart(inclinacion_peligrosa ~ ., data = train_data, method = "class")
# Realizar predicciones en el conjunto de test
test_data$prediction_class <- predict(model, test_data, type = "class")
# Asegurarse de que ambas columnas son factores con los mismos niveles
test_data$prediction_class <- factor(test_data$prediction_class, levels = c(0, 1))
# Calcular la matriz de confusión
cm <- confusionMatrix(test_data$prediction_class, test_data$inclinacion_peligrosa, positive = "1")
# Guardar las métricas
accuracy_list <- c(accuracy_list, cm$overall["Accuracy"])
precision_list <- c(precision_list, cm$byClass["Precision"])
sensitivity_list <- c(sensitivity_list, cm$byClass["Sensitivity"])
specificity_list <- c(specificity_list, cm$byClass["Specificity"])
}
# Calcular media y desviación estándar de cada métrica
results <- list(
accuracy_mean = mean(accuracy_list, na.rm = TRUE),
accuracy_sd = sd(accuracy_list, na.rm = TRUE),
precision_mean = mean(precision_list, na.rm = TRUE),
precision_sd = sd(precision_list, na.rm = TRUE),
sensitivity_mean = mean(sensitivity_list, na.rm = TRUE),
sensitivity_sd = sd(sensitivity_list, na.rm = TRUE),
specificity_mean = mean(specificity_list, na.rm = TRUE),
specificity_sd = sd(specificity_list, na.rm = TRUE)
)
return(results)
}
library(rpart)
library(caret)
library(dplyr)
cross_validation <- function(data, folds) {
# Asegurarse de que no hay NA en la variable de respuesta
data <- na.omit(data)
# Crear folds manteniendo la proporción de clases
set.seed(123) # Para reproducibilidad
folds_indices <- createDataPartition(data$inclinacion_peligrosa, p = 0.8, list = FALSE, times = folds)
# Inicializar listas para guardar las métricas de cada fold
accuracy_list <- c()
precision_list <- c()
sensitivity_list <- c()
specificity_list <- c()
for (i in seq_along(folds_indices)) {
# Separar los índices de test y entrenamiento para este fold
test_indices <- folds_indices[[i]]
train_data <- data[-test_indices, ]
test_data <- data[test_indices, ]
# Eliminar la columna 'ultima_modificacion'
train_data <- train_data %>% select(-ultima_modificacion)
test_data <- test_data %>% select(-ultima_modificacion)
# Asegurarse de que 'inclinacion_peligrosa' es un factor en ambos conjuntos
train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa, levels = c(0, 1))
test_data$inclinacion_peligrosa <- factor(test_data$inclinacion_peligrosa, levels = c(0, 1))
# Verificar niveles en ambos conjuntos
print(paste("Niveles en entrenamiento:", toString(levels(train_data$inclinacion_peligrosa))))
print(paste("Niveles en test:", toString(levels(test_data$inclinacion_peligrosa))))
# Entrenar el modelo de árbol de decisión
model <- rpart(inclinacion_peligrosa ~ ., data = train_data, method = "class")
# Realizar predicciones en el conjunto de test
test_data$prediction_class <- predict(model, test_data, type = "class")
# Asegurarse de que ambas columnas son factores con los mismos niveles
test_data$prediction_class <- factor(test_data$prediction_class, levels = levels(train_data$inclinacion_peligrosa))
# Calcular la matriz de confusión
cm <- confusionMatrix(test_data$prediction_class, test_data$inclinacion_peligrosa, positive = "1")
# Guardar las métricas
accuracy_list <- c(accuracy_list, cm$overall["Accuracy"])
precision_list <- c(precision_list, cm$byClass["Precision"])
sensitivity_list <- c(sensitivity_list, cm$byClass["Sensitivity"])
specificity_list <- c(specificity_list, cm$byClass["Specificity"])
}
# Calcular media y desviación estándar de cada métrica
results <- list(
accuracy_mean = mean(accuracy_list, na.rm = TRUE),
accuracy_sd = sd(accuracy_list, na.rm = TRUE),
precision_mean = mean(precision_list, na.rm = TRUE),
precision_sd = sd(precision_list, na.rm = TRUE),
sensitivity_mean = mean(sensitivity_list, na.rm = TRUE),
sensitivity_sd = sd(sensitivity_list, na.rm = TRUE),
specificity_mean = mean(specificity_list, na.rm = TRUE),
specificity_sd = sd(specificity_list, na.rm = TRUE)
)
return(results)
}
# Ejecutar la validación cruzada con el dataframe limpio
resultados <- cross_validation(datos_80, 10)
library(rpart)
library(caret)
library(dplyr)
cross_validation <- function(data, folds) {
# Asegurarse de que no hay NA en la variable de respuesta
data <- na.omit(data)
# Crear folds manteniendo la proporción de clases
set.seed(123) # Para reproducibilidad
folds_indices <- createDataPartition(data$inclinacion_peligrosa, p = 0.8, list = FALSE, times = folds)
# Inicializar listas para guardar las métricas de cada fold
accuracy_list <- c()
precision_list <- c()
sensitivity_list <- c()
specificity_list <- c()
for (i in seq_along(folds_indices)) {
# Separar los índices de test y entrenamiento para este fold
test_indices <- folds_indices[[i]]
train_data <- data[-test_indices, ]
test_data <- data[test_indices, ]
# Eliminar la columna 'ultima_modificacion'
train_data <- train_data %>% select(-ultima_modificacion)
test_data <- test_data %>% select(-ultima_modificacion)
# Asegurarse de que 'inclinacion_peligrosa' es un factor en ambos conjuntos
train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa, levels = c(0, 1))
test_data$inclinacion_peligrosa <- factor(test_data$inclinacion_peligrosa, levels = c(0, 1))
# Entrenar el modelo de árbol de decisión
model <- rpart(inclinacion_peligrosa ~ ., data = train_data, method = "class")
# Realizar predicciones en el conjunto de test
test_data$prediction_class <- predict(model, test_data, type = "class")
# Asegurarse de que ambas columnas son factores con los mismos niveles
test_data$prediction_class <- factor(test_data$prediction_class, levels = levels(train_data$inclinacion_peligrosa))
# Calcular la matriz de confusión
cm <- confusionMatrix(test_data$prediction_class, test_data$inclinacion_peligrosa, positive = "1")
# Guardar las métricas
accuracy_list <- c(accuracy_list, cm$overall["Accuracy"])
precision_list <- c(precision_list, cm$byClass["Precision"])
sensitivity_list <- c(sensitivity_list, cm$byClass["Sensitivity"])
specificity_list <- c(specificity_list, cm$byClass["Specificity"])
}
# Calcular media y desviación estándar de cada métrica
results <- list(
accuracy_mean = mean(accuracy_list, na.rm = TRUE),
accuracy_sd = sd(accuracy_list, na.rm = TRUE),
precision_mean = mean(precision_list, na.rm = TRUE),
precision_sd = sd(precision_list, na.rm = TRUE),
sensitivity_mean = mean(sensitivity_list, na.rm = TRUE),
sensitivity_sd = sd(sensitivity_list, na.rm = TRUE),
specificity_mean = mean(specificity_list, na.rm = TRUE),
specificity_sd = sd(specificity_list, na.rm = TRUE)
)
return(results)
}
# Ejecutar la validación cruzada con el dataframe limpio
resultados <- cross_validation(datos_80, 10)
print(resultados)
# Función para crear los folds
create_folds <- function(data, folds) {
# Crear folds manteniendo la proporción de clases
set.seed(123) # Para reproducibilidad
folds_indices <- createDataPartition(data$inclinacion_peligrosa, p = 0.8, list = FALSE, times = folds)
# Convertir los índices a una lista con nombres Fold1, Fold2, etc.
folds_list <- setNames(lapply(seq_along(folds_indices), function(i) folds_indices[[i]]), paste0("Fold", seq_along(folds_indices)))
return(folds_list)
}
library(rpart)
library(caret)
library(dplyr)
cross_validation <- function(data, folds) {
# Crear los folds usando la función create_folds
folds_indices <- create_folds(data, folds)
# Inicializar listas para guardar las métricas de cada fold
accuracy_list <- c()
precision_list <- c()
sensitivity_list <- c()
specificity_list <- c()
for (i in seq_along(folds_indices)) {
# Separar los índices de test y entrenamiento para este fold
test_indices <- folds_indices[[i]]
train_data <- data[-test_indices, ]
test_data <- data[test_indices, ]
# Eliminar la columna 'ultima_modificacion'
train_data <- train_data %>% select(-ultima_modificacion)
test_data <- test_data %>% select(-ultima_modificacion)
# Asegurarse de que 'inclinacion_peligrosa' es un factor en ambos conjuntos
train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa, levels = c(0, 1))
test_data$inclinacion_peligrosa <- factor(test_data$inclinacion_peligrosa, levels = c(0, 1))
# Entrenar el modelo de árbol de decisión
model <- rpart(inclinacion_peligrosa ~ ., data = train_data, method = "class")
# Realizar predicciones en el conjunto de test
test_data$prediction_class <- predict(model, test_data, type = "class")
# Asegurarse de que ambas columnas son factores con los mismos niveles
test_data$prediction_class <- factor(test_data$prediction_class, levels = levels(train_data$inclinacion_peligrosa))
# Calcular la matriz de confusión
cm <- confusionMatrix(test_data$prediction_class, test_data$inclinacion_peligrosa, positive = "1")
# Guardar las métricas
accuracy_list <- c(accuracy_list, cm$overall["Accuracy"])
precision_list <- c(precision_list, cm$byClass["Precision"])
sensitivity_list <- c(sensitivity_list, cm$byClass["Sensitivity"])
specificity_list <- c(specificity_list, cm$byClass["Specificity"])
}
# Calcular media y desviación estándar de cada métrica
results <- list(
accuracy_mean = mean(accuracy_list, na.rm = TRUE),
accuracy_sd = sd(accuracy_list, na.rm = TRUE),
precision_mean = mean(precision_list, na.rm = TRUE),
precision_sd = sd(precision_list, na.rm = TRUE),
sensitivity_mean = mean(sensitivity_list, na.rm = TRUE),
sensitivity_sd = sd(sensitivity_list, na.rm = TRUE),
specificity_mean = mean(specificity_list, na.rm = TRUE),
specificity_sd = sd(specificity_list, na.rm = TRUE)
)
return(results)
}
# Ejecutar la validación cruzada con el dataframe limpio
resultados_cross_validation <- cross_validation(datos_80, 10)
library(rpart)
library(caret)
library(dplyr)
cross_validation <- function(data, folds) {
# Crear folds manteniendo la proporción de clases
set.seed(123) # Para reproducibilidad
folds_indices <- create_folds(data, folds)
# Inicializar listas para guardar las métricas de cada fold
accuracy_list <- c()
precision_list <- c()
sensitivity_list <- c()
specificity_list <- c()
for (i in seq_along(folds_indices)) {
# Separar los índices de test y entrenamiento para este fold
test_indices <- folds_indices[[i]]
train_data <- data[-test_indices, ]
test_data <- data[test_indices, ]
# Eliminar la columna 'ultima_modificacion'
train_data <- train_data %>% select(-ultima_modificacion)
test_data <- test_data %>% select(-ultima_modificacion)
# Asegurarse de que 'inclinacion_peligrosa' es un factor en ambos conjuntos
train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa, levels = c(0, 1))
test_data$inclinacion_peligrosa <- factor(test_data$inclinacion_peligrosa, levels = c(0, 1))
# Asegurarse de que los niveles de 'especie' son los mismos en ambos conjuntos
train_data$especie <- factor(train_data$especie)
test_data$especie <- factor(test_data$especie, levels = levels(train_data$especie))
# Entrenar el modelo de árbol de decisión
model <- rpart(inclinacion_peligrosa ~ ., data = train_data, method = "class")
# Realizar predicciones en el conjunto de test
test_data$prediction_class <- predict(model, test_data, type = "class")
# Asegurarse de que ambas columnas son factores con los mismos niveles
test_data$prediction_class <- factor(test_data$prediction_class, levels = levels(train_data$inclinacion_peligrosa))
# Calcular la matriz de confusión
cm <- confusionMatrix(test_data$prediction_class, test_data$inclinacion_peligrosa, positive = "1")
# Guardar las métricas
accuracy_list <- c(accuracy_list, cm$overall["Accuracy"])
precision_list <- c(precision_list, cm$byClass["Precision"])
sensitivity_list <- c(sensitivity_list, cm$byClass["Sensitivity"])
specificity_list <- c(specificity_list, cm$byClass["Specificity"])
}
# Calcular media y desviación estándar de cada métrica
results <- list(
accuracy_mean = mean(accuracy_list, na.rm = TRUE),
accuracy_sd = sd(accuracy_list, na.rm = TRUE),
precision_mean = mean(precision_list, na.rm = TRUE),
precision_sd = sd(precision_list, na.rm = TRUE),
sensitivity_mean = mean(sensitivity_list, na.rm = TRUE),
sensitivity_sd = sd(sensitivity_list, na.rm = TRUE),
specificity_mean = mean(specificity_list, na.rm = TRUE),
specificity_sd = sd(specificity_list, na.rm = TRUE)
)
return(results)
}
# Ejecutar la validación cruzada con el dataframe limpio
resultados_cross_validation <- cross_validation(datos_80, 10)
